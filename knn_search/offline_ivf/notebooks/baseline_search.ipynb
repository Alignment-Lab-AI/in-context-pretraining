{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1449570\n",
      "CPU times: user 5.62 s, sys: 22.8 s, total: 28.5 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import faiss\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/data/home/marialomeli/faiss_improvements/offline_ivf\")\n",
    "from generate_config import read_embeddings\n",
    "index_path=\"/fsx-checkpoints/marialomeli/offline_faiss/contriever/github/IVF32768_PQ256.faissindex\"\n",
    "index_ivf=faiss.read_index(index_path) #TODO: need to add to gpu\n",
    "index_ivf.nprobe = 64\n",
    "#res = faiss.StandardGpuResources()\n",
    "#gpu_index_ivf = faiss.index_cpu_to_gpu(res, 0, index_ivf)\n",
    "# load embeddings\n",
    "root=\"/fsx-instruct-opt/swj0419/llama_data/embed/tokenizer-facebook_contriever_msmarco/seq_len-5100/chunk_len-510/model-facebook_contriever_msmarco/\"\n",
    "file_names = [f\"github_oss.chunk.{i:02}.jsonl.lz4.npy\" for i in range(32)]\n",
    "file_names=[\"github_oss.chunk.00.jsonl.lz4.npy\"]\n",
    "d = 768\n",
    "k=50\n",
    "\n",
    "for file in file_names:\n",
    "    fp=root+file\n",
    "    _,vecs = read_embeddings(fp)\n",
    "    vecs_small=vecs[:100,:]\n",
    "    D, I = index_ivf.search(vecs_small, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1075441.4212500001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CPU search\n",
    "small_time=18.6\n",
    "num_vecs=100\n",
    "big_num_vecs=46255545/8\n",
    "#small_time-num_vecs\n",
    "#x-big_num_vecs\n",
    "x=big_num_vecs*small_time/num_vecs\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.447238671875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1075441.4212500001/3600/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2319.5711610987782"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GPU search\n",
    "small_time=0.040117502212524414\n",
    "num_vecs=100\n",
    "big_num_vecs=46255545/8\n",
    "#small_time-num_vecs\n",
    "#x-big_num_vecs\n",
    "x=big_num_vecs*small_time/num_vecs\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18556.8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2319.6*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "File bigann_small0000000000.npy is saved!\n",
      "1\n",
      "File bigann_small0000000001.npy is saved!\n",
      "2\n",
      "File bigann_small0000000002.npy is saved!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from faiss.contrib.datasets import DatasetBigANN\n",
    "#this will only work on the fair cluster\n",
    "data=DatasetBigANN()\n",
    "count=0\n",
    "xb=data.database_iterator(bs=1_000)\n",
    "for i,data_batch in enumerate(xb):\n",
    "    if i>2:\n",
    "        break\n",
    "    print(i)\n",
    "    filename=f\"bigann_small{(i):010}.npy\"\n",
    "    np.save(filename,data_batch)\n",
    "    print(f\"File {filename} is saved!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000000, 128)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_file=f\"/checkpoint/marialomeli/big_ann_data/bigann0000000000.npy\"\n",
    "vec=np.load(test_file,mmap_mode='r')\n",
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 128)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from faiss.contrib.datasets import DatasetBigANN\n",
    "#this will only work on the fair cluster\n",
    "data=DatasetBigANN()\n",
    "count=0\n",
    "xq=data.get_queries()\n",
    "xq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "index_shard_path=\"/checkpoint/marialomeli/offline_faiss/bigann/bigann_1B\"\n",
    "queries_path=\"/checkpoint/marialomeli/big_ann_data/\"\n",
    "# each machine will load an index shard and a query batches and conduct search\n",
    "# for example\n",
    "i=0\n",
    "index_file = index_shard_path+f\"/IVF1048576_PQ64.faissindex\"\n",
    "index = faiss.read_index(index_file)\n",
    "index.by_residual=True\n",
    "query_batch = np.load(queries_path+f\"bigann{(i):010}.npy\",mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6250000, 128)\n"
     ]
    }
   ],
   "source": [
    "#move index to GPU\n",
    "res = faiss.StandardGpuResources()\n",
    "co = faiss.GpuClonerOptions()\n",
    "co.useFloat16 = True\n",
    "small_batch=query_batch.shape[0]//8\n",
    "small_query_batch = query_batch[:small_batch,]\n",
    "print(small_query_batch.shape)\n",
    "#50,000/8 per small batch\n",
    "index_gpu = faiss.index_cpu_to_gpu(res,0,index,co)\n",
    "D,I = index_gpu.search(query_batch,k=2)\n",
    "I[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "def xbin_mmap(fname, dtype, maxn=-1):\n",
    "    \"\"\" mmap the competition file format for a given type of items \"\"\"\n",
    "    n, d = map(int, np.fromfile(fname, dtype=\"uint32\", count=2))\n",
    "\n",
    "    # HACK - to handle improper header in file for private deep-1B\n",
    "    # if override_d and override_d != d:\n",
    "    #    print(\"Warning: xbin_mmap map returned d=%s, but overridig with %d\" % (d, override_d))\n",
    "    #    d = override_d\n",
    "    # HACK\n",
    "\n",
    "    assert os.stat(fname).st_size == 8 + n * d * np.dtype(dtype).itemsize\n",
    "    if maxn > 0:\n",
    "        n = min(n, maxn)\n",
    "    return np.memmap(fname, dtype=dtype, mode=\"r\", offset=8, shape=(n, d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssn++ \n",
    "# read with https://github.com/harsha-simhadri/big-ann-benchmarks/blob/main/benchmark/dataset_io.py#L94 with dtype=â€œuint8\"\n",
    "filepath=\"/datasets01/big-ann-challenge-data/FB_ssnpp/FB_ssnpp_database.u8bin\"\n",
    "dtype=\"uint8\"\n",
    "\n",
    "data=xbin_mmap(fname=filepath,dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000000, 256)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch=50000000\n",
    "for i in range(20):\n",
    "    if i>2:\n",
    "        break\n",
    "    xb_batch=data[i*data_batch:(i+1)*data_batch,:] #[0:49000000],[50000000:99000000],[100000000:149000000]\n",
    "    print(xb_batch.shape)\n",
    "    filename=f\"/checkpoint/marialomeli/ssnpp_data/ssnpp_{(i):010}.npy\"\n",
    "    np.save(filename,xb_batch)\n",
    "    print(f\"File {filename} is saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 6427.7863636363645 std: 7930.9687639540025 min: 3887.4 max 42415.2\n"
     ]
    }
   ],
   "source": [
    "# ssnpp benchmarks\n",
    "# IVF8192_PQ128_np128\n",
    "IVF8192_PQ128_np128_times=[3887.4,4046.7,4328.8,4519.2,42415.2,7611.4,4477,3951.7,3947.7,3948.5,7611.4,4477.0,3951.7,3947.7,3948.5,3904.1,5906.1,5429.9,5769,4033.9,3893.7,5404.7]\n",
    "import numpy as np\n",
    "print(\"mean:\",np.mean(IVF8192_PQ128_np128_times),\"std:\",np.std(IVF8192_PQ128_np128_times),\"min:\",min(IVF8192_PQ128_np128_times),\"max\",max(IVF8192_PQ128_np128_times))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 10003.9625 std: 11720.347793414398 min: 4873.4 max 52393.9\n"
     ]
    }
   ],
   "source": [
    "IVF16384_PQ128_np256_times=[4888.8,4881.7,7581.5,5272.1,5237.8,5110.3,5269.3,5170.5,5304.9,5584.2,4896.1,4873.4,17986.5,7590.9,18021.5, 52393.9]\n",
    "print(\"mean:\",np.mean(IVF16384_PQ128_np256_times),\"std:\",np.std(IVF16384_PQ128_np256_times),\"min:\",min(IVF16384_PQ128_np256_times),\"max\",max(IVF16384_PQ128_np256_times))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 16020.442857142856 std: 8875.992270247682 min: 7318.0 max 34726.0\n"
     ]
    }
   ],
   "source": [
    "IVF32768_PQ128_np512_times=[12076.9,24584.7,12076.9, 24584.7, 32799.3, 11658.4,9569.8,17956.1,11380.4,29773.2,26810.1,7376.7, 18612.9, 7318.0,7842.5,8334.9,11149.1,8212.9,11840.8,34726.0, 7745]\n",
    "print(\"mean:\",np.mean(IVF32768_PQ128_np512_times),\"std:\",np.std(IVF32768_PQ128_np512_times),\"min:\",min(IVF32768_PQ128_np512_times),\"max\",max(IVF32768_PQ128_np512_times))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 5937.995 std: 32.38220923593702 min: 5846.2 max 5969.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "IVF8192_PQ32_np128_times=[5957.5,5966.2,5968.2,5956.6,5969.8,5961,5853.4,5846.2,5929.4,5920.2,5927.5,5950.7,5942.3,5945.4,5949.7,5941.4,5947,5926.7,5953.2,5947.5]\n",
    "print(\"mean:\",np.mean(IVF8192_PQ32_np128_times),\"std:\",np.std(IVF8192_PQ32_np128_times),\"min:\",min(IVF8192_PQ32_np128_times),\"max\",max(IVF8192_PQ32_np128_times))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 8500.926943805694 std: 28.72199385838948 min: 8452.929758787155 max 8534.295\n"
     ]
    }
   ],
   "source": [
    "#online search IVF8192_PQ32_np128\n",
    "IVF8192_PQ32_np128_online_times={\"batch0\":8534.295,\"batch2\":8501.273007392883,\"batch5\": 8456.743677854538,\"batch6\": 8520.941766500473,\"batch7\":8516.610247850418,\"batch8\": 8461.659716844559,\"batch9\": 8515.047048330307,\"batch10\": 8525.11458826065,\"batch11\":8519.585188150406,\"batch12\":8452.929758787155,\"batch13\":8529.626145124435,\"batch16\":8521.815571546555,\"batch17\":8457.880843639374,\"batch18\": 8517.069823026657,\"batch19\": 8483.311773777008}\n",
    "print(\"mean:\",np.mean(list(IVF8192_PQ32_np128_online_times.values())),\"std:\",np.std(list(IVF8192_PQ32_np128_online_times.values())),\"min:\",min(list(IVF8192_PQ32_np128_online_times.values())),\"max\",max(list(IVF8192_PQ32_np128_online_times.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 8038.120000000001 std: 2523.100273988333 min: 2001.1 max 9146.9\n"
     ]
    }
   ],
   "source": [
    "IVF16384_PQ32_np256_times=[9057.1,9092.2,8969.6,9121.5,9105.7,9128.6,9100.3,9110.4,9092.2,9090.0,9115.2,9115.2,9058.8,9110.1,9146.9,2001.1,2009.7,9134.1,9116.5,2087.2]\n",
    "print(\"mean:\",np.mean(IVF16384_PQ32_np256_times),\"std:\",np.std(IVF16384_PQ32_np256_times),\"min:\",min(IVF16384_PQ32_np256_times),\"max\",max(IVF16384_PQ32_np256_times))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('oivf_nightly_latest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "683cc96f21fe88ec73744201e680a988fad79b27c29191148ba1b1a99d6256df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
